{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "orderbook = \"data/GOOG_2012-06-21_34200000_57600000_orderbook_10.csv\"\n",
    "message = \"data/GOOG_2012-06-21_34200000_57600000_message_10.csv\"\n",
    "num_levels = 10\n",
    "\n",
    "header_list = []\n",
    "for i in range(num_levels):\n",
    "    header_list = header_list + [\"Pa%d\"%(i+1),\"Va%d\"%(i+1),\"Pb%d\"%(i+1),\"Vb%d\"%(i+1)]\n",
    "df_orderbook = pd.read_csv(orderbook,header=None,names=header_list)\n",
    "\n",
    "df_message = pd.read_csv(message,usecols = [0,1,3,4,5], names=['time', 'type','size','price','direction'])\n",
    "df_message.index = pd.Timestamp(datetime.date.today()) + pd.TimedeltaIndex(df_message.time, unit='s')\n",
    "df_orderbook.index = df_message.index\n",
    "\n",
    "def labelling(a):\n",
    "    if a > 0:\n",
    "        b = 1\n",
    "    else:\n",
    "        b= 0\n",
    "    return b\n",
    "\n",
    "# Spreads and mid-prices\n",
    "def feature_v2(num_levels,df): # 20\n",
    "    for i in range(1,num_levels+1):\n",
    "        df[\"spread%d\"%(i)] = df[\"Pa%d\"%(i)] - df[\"Pb%d\"%(i)]\n",
    "        df[\"midprice%d\"%(i)] = (df[\"Pa%d\"%(i)] + df[\"Pb%d\"%(i)])/2\n",
    "    return df\n",
    "\n",
    "def feature_v3(num_levels,df): # 20 - 2\n",
    "    for i in range(1, num_levels):\n",
    "        df[\"PA_diff%d\"%(i)] = df[\"Pa%d\"%(i+1)] - df[\"Pa%d\"%(i)]\n",
    "        df[\"PB_diff%d\"%(i)] = df[\"Pb%d\"%(i)] - df[\"Pb%d\"%(i+1)]\n",
    "    return df\n",
    "\n",
    "def feature_v4(num_levels,df): # 4\n",
    "    lst = [\"Pa%d\"%(i+1) for i in range(num_levels)]\n",
    "    df[\"Pa_mean\"] = df[df.columns.intersection(lst)].sum(axis=1)    \n",
    "    \n",
    "    lst = [\"Pb%d\"%(i+1) for i in range(num_levels)]\n",
    "    df[\"Pb_mean\"] = df[df.columns.intersection(lst)].sum(axis=1)\n",
    "    \n",
    "    lst = [\"Va%d\"%(i+1) for i in range(num_levels)]\n",
    "    df[\"Va_mean\"] = df[df.columns.intersection(lst)].sum(axis=1)\n",
    "    \n",
    "    lst = [\"Vb%d\"%(i+1) for i in range(num_levels)]\n",
    "    df[\"Vb_mean\"] = df[df.columns.intersection(lst)].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "def feature_v5(num_levels,df): # 20\n",
    "    for i in range(num_levels): #\n",
    "        df[\"pri_accum_diff%d\"%(i+1)] = 0\n",
    "        df[\"vol_accum_diff%d\"%(i+1)] = 0\n",
    "        for k in range(i):\n",
    "            df[\"pri_accum_diff%d\"%(i+1)] += (df[\"Pa%d\"%(k+1)] - df[\"Pb%d\"%(k+1)])\n",
    "            df[\"vol_accum_diff%d\"%(i+1)] += (df[\"Va%d\"%(i+1)] - df[\"Vb%d\"%(i+1)])\n",
    "    return df\n",
    "\n",
    "def normalize_input(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    # Fit on training set only.\n",
    "    scaler.fit(X_train)\n",
    "    # Apply transform to both the training set and the test set.\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "df_orderbook = feature_v2(num_levels,df_orderbook)\n",
    "df_orderbook = feature_v3(num_levels,df_orderbook)\n",
    "df_orderbook = feature_v4(num_levels,df_orderbook)\n",
    "df_orderbook = feature_v5(num_levels,df_orderbook)\n",
    "\n",
    "# df_orderbook.head()\n",
    "# window = 1000\n",
    "# df_orderbook['midprice_win%d'%window] = df_orderbook.rolling(window).agg({'midprice1':'mean'})\n",
    "# df_orderbook['midprice_win%d'%window].plot()\n",
    "\n",
    "# window = '1MIN'\n",
    "# df_orderbook['midprice_average'+window] = df_orderbook.rolling('1MIN').agg({'midprice1':'mean'})\n",
    "# df_orderbook['midprice_average'+window].plot()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "def model_scoring(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(\"======================================\")\n",
    "    print(\"======Model Performance===============\")\n",
    "\n",
    "    print(\"the accuracy is: \", accuracy)\n",
    "    print(\"the precision isï¼š\",precision)\n",
    "    print(\"the recall is: \", recall)\n",
    "    print(\"the f1 score is: \", f1)\n",
    "    print(\"confution matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "def split_sequence(X_sequence, y_sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(0, len(X_sequence)-n_steps+1):\n",
    "        seq_x, seq_y = X_sequence[i: i+n_steps], y_sequence[i + n_steps - 1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "####################################################################\n",
    "####################################################################\n",
    "\n",
    "df_orderbook1s = df_orderbook.resample('1S').first()\n",
    "df_orderbook1s[\"price_10min\"] = df_orderbook1s['midprice1'].shift(-600)\n",
    "df_orderbook1s.dropna(inplace=True)\n",
    "df_orderbook1s[\"price_change\"] = df_orderbook1s[\"price_10min\"] - df_orderbook1s['midprice1']\n",
    "\n",
    "X = df_orderbook1s.drop(['price_10min', 'price_change'], axis=1).values\n",
    "y = np.array(list(map(labelling, df_orderbook1s[\"price_change\"])))\n",
    "####################################################################\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10743, 102)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10743, 102)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10743,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 50)                30600     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                1632      \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 32,265\n",
      "Trainable params: 32,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 5222 samples, validate on 2239 samples\n",
      "Epoch 1/30\n",
      "5222/5222 [==============================] - 15s 3ms/sample - loss: 0.6289 - accuracy: 0.6760 - val_loss: 0.6144 - val_accuracy: 0.6766\n",
      "Epoch 2/30\n",
      "5222/5222 [==============================] - 12s 2ms/sample - loss: 0.6049 - accuracy: 0.6812 - val_loss: 0.6069 - val_accuracy: 0.6842\n",
      "Epoch 3/30\n",
      "5222/5222 [==============================] - 11s 2ms/sample - loss: 0.5839 - accuracy: 0.6978 - val_loss: 0.5936 - val_accuracy: 0.6896\n",
      "Epoch 4/30\n",
      "5222/5222 [==============================] - 11s 2ms/sample - loss: 0.5591 - accuracy: 0.7099 - val_loss: 0.5855 - val_accuracy: 0.6865\n",
      "Epoch 5/30\n",
      "5222/5222 [==============================] - 10s 2ms/sample - loss: 0.5338 - accuracy: 0.7285 - val_loss: 0.6116 - val_accuracy: 0.6967\n",
      "Epoch 6/30\n",
      "5222/5222 [==============================] - 11s 2ms/sample - loss: 0.5025 - accuracy: 0.7470 - val_loss: 0.5956 - val_accuracy: 0.6757\n",
      "Epoch 7/30\n",
      "5222/5222 [==============================] - 11s 2ms/sample - loss: 0.4697 - accuracy: 0.7677 - val_loss: 0.6085 - val_accuracy: 0.6802\n",
      "Epoch 8/30\n",
      "5222/5222 [==============================] - 10s 2ms/sample - loss: 0.4346 - accuracy: 0.7905 - val_loss: 0.6227 - val_accuracy: 0.6927\n",
      "Epoch 9/30\n",
      "5222/5222 [==============================] - 10s 2ms/sample - loss: 0.3877 - accuracy: 0.8165 - val_loss: 0.6579 - val_accuracy: 0.6766\n",
      "Epoch 10/30\n",
      "4480/5222 [========================>.....] - ETA: 1s - loss: 0.3493 - accuracy: 0.8395"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding, LSTM\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv1D\n",
    "######################################\n",
    "n_steps = 60\n",
    "# prepare train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_test = normalize_input(X_train, X_test)\n",
    "# split into samples\n",
    "X_train_steps, y_train_steps = split_sequence(X_train, y_train, n_steps)\n",
    "X_test_steps, y_test_steps = split_sequence(X_test, y_test, n_steps)\n",
    "######################################\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, 102)))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "######################################\n",
    "model.fit(X_train_steps, y_train_steps, \n",
    "          batch_size=32, epochs=30, validation_split=0.3)\n",
    "######################################\n",
    "y_pred = model.predict(X_test_steps)\n",
    "y_pred[y_pred>0.5] = 1\n",
    "y_pred[y_pred<0.5] = 0\n",
    "model_scoring(y_test_steps, y_pred)\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_test = normalize_input(X_train, X_test)\n",
    "clf = LogisticRegression(random_state=0, \n",
    "                         penalty='l2',\n",
    "                         solver='lbfgs',\n",
    "                         max_iter = 100000,\n",
    "                         multi_class='ovr').fit(X_train, y_train)\n",
    "print(\"training score.\", clf.score(X_train, y_train))\n",
    "print(\"testing score.\", clf.score(X_test, y_test))\n",
    "y_pred = clf.predict(X_test)\n",
    "model_scoring(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    " ######### XGBoost ########\n",
    "############################\n",
    "import xgboost as xgb\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_test = normalize_input(X_train, X_test)\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "model_scoring(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 99, 256)           1280      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 99, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 49, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 48, 256)           131328    \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 48, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 24, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 24, 128)           32896     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 24, 128)           0         \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 24, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 12, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 362,369\n",
      "Trainable params: 362,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    " ########## CNN ##########\n",
    "############################\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv1D,  MaxPooling1D\n",
    "import pickle\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_test = normalize_input(X_train, X_test)\n",
    "model = Sequential()\n",
    "model.add(Conv1D(256, 4, input_shape=(102, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(256, 2))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(128, 1))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train.reshape((X_train.shape[0],X_train.shape[1],1)), np.asarray(y_train), \n",
    "          batch_size=32, epochs=50, validation_split=0.3)\n",
    "y_pred = model.predict(X_test.reshape((X_test.shape[0],X_test.shape[1],1)))\n",
    "y_pred[y_pred>0.5] = 1\n",
    "y_pred[y_pred<0.5] = 0\n",
    "model_scoring(y_test, y_pred.reshape((-1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/examples/imdb_cnn_lstm/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
